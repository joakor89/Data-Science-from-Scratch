{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3945f244",
   "metadata": {},
   "source": [
    "# Chapter 16 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736df0b",
   "metadata": {},
   "source": [
    "### The Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9be2362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c530f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [(0.7,48000,1),(1.9,48000,0),(2.5,60000,1),(4.2,63000,0),(6,76000,0),(6.5,69000,0),(7.5,76000,0),(8.1,88000,0),(8.7,83000,1),(10,83000,1),(0.8,43000,0),(1.8,60000,0),(10,79000,1),(6.1,76000,0),(1.4,50000,0),(9.1,92000,0),(5.8,75000,0),(5.2,69000,0),(1,56000,0),(6,67000,0),(4.9,74000,0),(6.4,63000,1),(6.2,82000,0),(3.3,58000,0),(9.3,90000,1),(5.5,57000,1),(9.1,102000,0),(2.4,54000,0),(8.2,65000,1),(5.3,82000,0),(9.8,107000,0),(1.8,64000,0),(0.6,46000,1),(0.8,48000,0),(8.6,84000,1),(0.6,45000,0),(0.5,30000,1),(7.3,89000,0),(2.5,48000,1),(5.6,76000,0),(7.4,77000,0),(2.7,56000,0),(0.7,48000,0),(1.2,42000,0),(0.2,32000,1),(4.7,56000,1),(2.8,44000,1),(7.6,78000,0),(1.1,63000,0),(8,79000,1),(2.7,56000,0),(6,52000,1),(4.6,56000,0),(2.5,51000,0),(5.7,71000,0),(2.9,65000,0),(1.1,33000,1),(3,62000,0),(4,71000,0),(2.4,61000,0),(7.5,75000,0),(9.7,81000,1),(3.2,62000,0),(7.9,88000,0),(4.7,44000,1),(2.5,55000,0),(1.6,41000,0),(6.7,64000,1),(6.9,66000,1),(7.9,78000,1),(8.1,102000,0),(5.3,48000,1),(8.5,66000,1),(0.2,56000,0),(6,69000,0),(7.5,77000,0),(8,86000,0),(4.4,68000,0),(4.9,75000,0),(1.5,60000,0),(2.2,50000,0),(3.4,49000,1),(4.2,70000,0),(7.7,98000,0),(8.2,85000,0),(5.4,88000,0),(0.1,46000,0),(1.5,37000,0),(6.3,86000,0),(3.7,57000,0),(8.4,85000,0),(2,42000,0),(5.8,69000,1),(2.7,64000,0),(3.1,63000,0),(1.9,48000,0),(10,72000,1),(0.2,45000,0),(8.6,95000,0),(1.5,64000,0),(9.8,95000,0),(5.3,65000,0),(7.5,80000,0),(9.9,91000,0),(9.7,50000,1),(2.8,68000,0),(3.6,58000,0),(3.9,74000,0),(4.4,76000,0),(2.5,49000,0),(7.2,81000,0),(5.2,60000,1),(2.4,62000,0),(8.9,94000,0),(2.4,63000,0),(6.8,69000,1),(6.5,77000,0),(7,86000,0),(9.4,94000,0),(7.8,72000,1),(0.2,53000,0),(10,97000,0),(5.5,65000,0),(7.7,71000,1),(8.1,66000,1),(9.8,91000,0),(8,84000,0),(2.7,55000,0),(2.8,62000,0),(9.4,79000,0),(2.5,57000,0),(7.4,70000,1),(2.1,47000,0),(5.3,62000,1),(6.3,79000,0),(6.8,58000,1),(5.7,80000,0),(2.2,61000,0),(4.8,62000,0),(3.7,64000,0),(4.1,85000,0),(2.3,51000,0),(3.5,58000,0),(0.9,43000,0),(0.9,54000,0),(4.5,74000,0),(6.5,55000,1),(4.1,41000,1),(7.1,73000,0),(1.1,66000,0),(9.1,81000,1),(8,69000,1),(7.3,72000,1),(3.3,50000,0),(3.9,58000,0),(2.6,49000,0),(1.6,78000,0),(0.7,56000,0),(2.1,36000,1),(7.5,90000,0),(4.8,59000,1),(8.9,95000,0),(6.2,72000,0),(6.3,63000,0),(9.1,100000,0),(7.3,61000,1),(5.6,74000,0),(0.5,66000,0),(1.1,59000,0),(5.1,61000,0),(6.2,70000,0),(6.6,56000,1),(6.3,76000,0),(6.5,78000,0),(5.1,59000,0),(9.5,74000,1),(4.5,64000,0),(2,54000,0),(1,52000,0),(4,69000,0),(6.5,76000,0),(3,60000,0),(4.5,63000,0),(7.8,70000,0),(3.9,60000,1),(0.8,51000,0),(4.2,78000,0),(1.1,54000,0),(6.2,60000,0),(2.9,59000,0),(2.1,52000,0),(8.2,87000,0),(4.8,73000,0),(2.2,42000,1),(9.1,98000,0),(6.5,84000,0),(6.9,73000,0),(5.1,72000,0),(9.1,69000,1),(9.8,79000,1),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "370bddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [list(row) for row in tuples]\n",
    "\n",
    "xs = [[1.0] + row[:2] for row in data]\n",
    "ys = [row[2] for row in data]\n",
    "\n",
    "Vector = List[float]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac8ee5",
   "metadata": {},
   "source": [
    "#### Math & Rescale Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba004a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_sum(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "assert vector_sum([[1, 2], [3, 4], [5, 6], [7, 8]]) == [16, 20]\n",
    "\n",
    "def scalar_multiply(c: float, v: Vector) -> Vector:\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "assert scalar_multiply(2, [1, 2, 3]) == [2, 4, 6]\n",
    "\n",
    "def vector_mean(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a44ac457",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_friends = [100.0,49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14,13,13,13,13,12,12,11,\n",
    "               10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,\n",
    "               9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,\n",
    "               6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,\n",
    "               4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,\n",
    "               2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "def mean(xs: List[float]) -> float:\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "mean(num_friends)   # 7.333333\n",
    "\n",
    "\n",
    "assert 7.3333 < mean(num_friends) < 7.3334\n",
    "\n",
    "\n",
    "def de_mean(xs: List[float]) -> List[float]:\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = mean(xs)\n",
    "    return [x - x_bar for x in xs]\n",
    "\n",
    "def variance(xs: List[float]) -> float:\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "# assert 81.54 < variance(num_friends) < 81.55\n",
    "\n",
    "import math\n",
    "\n",
    "def standard_deviation(xs: List[float]) -> float:\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    return math.sqrt(variance(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a72448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "assert dot([1, 2, 3], [4, 5, 6]) == 32  # 1 * 4 + 2 * 5 + 3 * 6\n",
    "\n",
    "###\n",
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "assert sum_of_squares([1, 2, 3]) == 14  # 1 * 1 + 2 * 2 + 3 * 3\n",
    "\n",
    "###\n",
    "def scale(data: List[Vector]) -> Tuple[Vector, Vector]:\n",
    "    \"\"\"returns the means and standard deviations for each position\"\"\"\n",
    "    dim = len(data[0])\n",
    "\n",
    "    means = vector_mean(data)\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "\n",
    "    return means, stdevs\n",
    "\n",
    "vectors = [[-3, -1, 1], [-1, 0, 1], [1, 1, 1]]\n",
    "means, stdevs = scale(vectors)\n",
    "assert means == [-1, 0, 1]\n",
    "assert stdevs == [2, 1, 0]\n",
    "\n",
    "###\n",
    "def rescale(data: List[Vector]) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = len(data[0])\n",
    "    means, stdevs = scale(data)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "\n",
    "    for v in rescaled:\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "\n",
    "    return rescaled\n",
    "\n",
    "means, stdevs = scale(rescale(vectors))\n",
    "assert means == [0, 0, 1]\n",
    "assert stdevs == [1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d319f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"Adds corresponding elements\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be the same length\"\n",
    "\n",
    "    return [v_i + w_i for v_i, w_i in zip(v, w)]\n",
    "\n",
    "assert add([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a76341",
   "metadata": {},
   "source": [
    "#### Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e269f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs: List[List[float]] = [[1.,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79f0a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3a40553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return predict(x, beta) - y\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = 30\n",
    "beta = [4, 4, 4]  # so prediction = 4 + 8 + 12 = 24\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "194b5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    err = error(x, y, beta)\n",
    "    return [2 * err * x_i for x_i in x]\n",
    "\n",
    "assert sqerror_gradient(x, y, beta) == [-12, -24, -36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8f3cfb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(xs: List[Vector],\n",
    "                      ys: List[float],\n",
    "                      learning_rate: float = 0.001,\n",
    "                      num_steps: int = 1000,\n",
    "                      batch_size: int = 1) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ecc97",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b754f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5731f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(xs: List[Vector],\n",
    "                      ys: List[float],\n",
    "                      learning_rate: float = 0.001,\n",
    "                      num_steps: int = 1000,\n",
    "                      batch_size: int = 1) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c277cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|███████████████████| 1000/1000 [00:01<00:00, 704.77it/s]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "rescaled_xs = rescale(xs)\n",
    "beta = least_squares_fit(rescaled_xs, ys, learning_rate, 1000, 1)\n",
    "predictions = [predict(x_i, beta) for x_i in rescaled_xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7bb5748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(predictions, ys)\n",
    "plt.xlabel(\"predicted\")\n",
    "plt.ylabel(\"actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab43733",
   "metadata": {},
   "source": [
    "### Logistic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24d70343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(c: float) -> float:\n",
    "    return 1.0 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89c34362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_prime(x: float) -> float:\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e6e724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_log_likelihood(x: Vector, y: float, beta: Vector) -> float:\n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x. beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73631860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(xs: list[Vector],\n",
    "                            ys: List[float],\n",
    "                            beta: Vector) -> float:\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81606863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_log_partial_j(x: Vector, y: float, beta: Vector, j: int) -> float:\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "212fd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_gradient(xs: List[Vector],\n",
    "                           ys: List[float],\n",
    "                           beta: Vector) -> Vector:\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6cb2ba",
   "metadata": {},
   "source": [
    "### Applying The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "98b49df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39c8ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = TypeVar('X')  # generic type to represent a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2b2fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "data = [n for n in range(1000)]\n",
    "train, test = split_data(data, 0.75)\n",
    "\n",
    "# The proportions should be correct\n",
    "assert len(train) == 750\n",
    "assert len(test) == 250\n",
    "\n",
    "# And the original data should be preserved (in some order)\n",
    "assert sorted(train + test) == data\n",
    "\n",
    "Y = TypeVar('Y')  # generic type to represent output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7eecad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(xs: List[X],\n",
    "                     ys: List[Y],\n",
    "                     test_pct: float) -> Tuple[List[X], List[X], List[Y], List[Y]]:\n",
    "    # Generate the indices and split them.\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train\n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "\n",
    "xs = [x for x in range(1000)]  # xs are 1 ... 1000\n",
    "ys = [2 * x for x in xs]       # each y_i is twice x_i\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.25)\n",
    "\n",
    "# Check that the proportions are correct\n",
    "assert len(x_train) == len(y_train) == 750\n",
    "assert len(x_test) == len(y_test) == 250\n",
    "\n",
    "# Check that the corresponding data points are paired correctly.\n",
    "assert all(y == 2 * x for x, y in zip(x_train, y_train))\n",
    "assert all(y == 2 * x for x, y in zip(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "970cff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e833b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = [random.random() for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7811240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tqdm.trange(5000) as t:\n",
    "#     for epoch in t:\n",
    "#         gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "#         beta = gradient_ste(beta, gradient, -learning_rate)\n",
    "#         loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "#         t.set_description(f\"loss: {loss:.3f} beta: {beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43a3ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means, stdevs = scale(xs)\n",
    "# beta_unscaled = [(beta[0]\n",
    "#                   - beta[1] * means[1] / stdevs[1]\n",
    "#                   - beta[2] * means[2] / stdevs[2]),\n",
    "#                   - beta[1] / stdevs[1],\n",
    "#                   - beta[2] / stdevs[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639cdd0",
   "metadata": {},
   "source": [
    "### Goodness of Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8a4562b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "\n",
    "# for x_i, y_i in zip(x_test, y_test):\n",
    "#     prediction = logistic(dot(beta, x_i))\n",
    "    \n",
    "#     if y_i == 1 and prediction >= 0.5:\n",
    "#         true_positives += 1\n",
    "#     elif y_i == 1:\n",
    "#         false_negatives += 1\n",
    "#     elif prediction >= 0.5:\n",
    "#         false_positives == 1\n",
    "#     else:\n",
    "#         true_negatives += 1\n",
    "        \n",
    "# precision = true_positives / (true_positives + false_positives)\n",
    "# recall = true_positives / (true_positives + false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7a0372cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = [logistic(do(beta, x_i)) for x_i in x_test]\n",
    "# plt.scatter(predictions, y_test, marker='+')\n",
    "# plt.xlabel(\"predicted probability\")\n",
    "# plt.ylabel(\"actual outcome\")\n",
    "# plt.title(\"Logistic Regression Predicted vs. Actual\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e2a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
